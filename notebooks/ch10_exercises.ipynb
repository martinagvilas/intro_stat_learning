{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "In the chapter, we mentioned the use of correlation-based distance and Euclidean distance as dissimilarity measures for hierarchical clustering. It turns out that these two measures are almost equivalent: if each observation has been centered to have mean zero and standard deviation one, and if we let rij denote the correlation between the ith and jth observations, then the quantity 1âˆ’rij is proportional to the squared Euclidean distance between the ith and jth observations. On the USArrests data, show that this proportionality holds. Hint: The Euclidean distance can be calculated using the dist() function, and correlations can be calculated using the cor() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load USA arrests data\n",
    "data_usarrests = pd.read_csv('../data/usa_arrest.csv')\n",
    "\n",
    "# Define X\n",
    "features = ['Murder', 'Assault', 'UrbanPop', 'Rape']\n",
    "X = data_usarrests.loc[:, features].values\n",
    "\n",
    "# Standarize data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we have transformed the data to have mean zero and std 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the columns means: \n",
      "[-7.10542736e-17  1.38777878e-16 -4.39648318e-16  8.59312621e-16]\n",
      "\n",
      "These are the columns std: \n",
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Take mean and std of each column (feature)\n",
    "mean = np.mean(X,axis=0)\n",
    "print(\"These are the columns means: \\n\" + str(mean))\n",
    "std = np.std(X, axis=0)\n",
    "print(\"\\nThese are the columns std: \\n\" + str(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the eucledian distance and the 1- corrlation values for the paiwise comparisons of each row in X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = pdist(X, metric='seuclidean') # calculate squared euclidean distance\n",
    "correlation = 1 - pdist(X, metric='correlation') # calculate 1 -correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take the ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The median is: 0.0\n"
     ]
    }
   ],
   "source": [
    "constant = correlation/euclidean\n",
    "median_constant = np.median(constant)\n",
    "\n",
    "proportion = correlation - median_constant*euclidean\n",
    "median_proportion = np.median(proportion)\n",
    "print('The median is: ' + str(median_proportion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Section 10.2.3, a formula for calculating PVE was given in Equation 10.8. We also saw that the PVE can be obtained using the sdev output of the prcomp() function.\n",
    "\n",
    "On the USArrests data, calculate PVE in two ways:\n",
    "- (a) Using the sdev output of the prcomp() function, as was done in Section 10.2.3.\n",
    "- (b) By applying Equation 10.8 directly. That is, use the prcomp() function to compute the principal component loadings. Then, use those loadings in Equation 10.8 to obtain the PVE.\n",
    "\n",
    "These two approaches should give the same results.\n",
    "\n",
    "Hint: You will only obtain the same results in (a) and (b) if the same data is used in both cases. For instance, if in (a) you performed prcomp() using centered and scaled variables, then you must center and scale the variables before applying Equation 10.3 in (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute PCA over scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA\n",
    "pca = PCA()\n",
    "pca_data = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the proportion of variance explained by each component using the output explained_variance_ratio_ of scikit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  PCA_1     PCA_2     PCA_3     PCA_4\n",
      "Variance_ratio  0.62006  0.247441  0.089141  0.043358\n"
     ]
    }
   ],
   "source": [
    "# Print explained variance by each component\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "columns_pca = ['PCA_1', 'PCA_2', 'PCA_3', 'PCA_4']\n",
    "variance_df = pd.DataFrame([variance_ratio], index = ['Variance_ratio'], columns=columns_pca)\n",
    "print(variance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the PVE using Equation 10.8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PCA loadings\n",
    "pca_loadings = (pca.components_).T\n",
    "pca_loadings_df = pd.DataFrame(pca_loadings, index=features, columns=columns_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA_0 PVE :0.6200603947873732\n",
      "PCA_1 PVE :0.24744128813496016\n",
      "PCA_2 PVE :0.0891407951452075\n",
      "PCA_3 PVE :0.0433575219324588\n"
     ]
    }
   ],
   "source": [
    "# Compute denominator (this is going to be the same for every pca)\n",
    "matrix_sqr = X**2 \n",
    "sum_denominator = np.sum(matrix_sqr)\n",
    "\n",
    "# Compute numerator for each pca\n",
    "for pca in range(0,len(columns_pca)):\n",
    "    \n",
    "    # import corresponding loadings\n",
    "    loadings = pca_loadings[:,pca]\n",
    "\n",
    "    # make sure to grab correct loadings (their sum of squares should approximate 1)\n",
    "    assert np.sum(loadings**2) > 0.99\n",
    "    \n",
    "    # compute numerator\n",
    "    matrix = X * loadings\n",
    "    sum_pred_n = np.sum(matrix, axis=1)\n",
    "    sum_pred_n_sqr = sum_pred_n ** 2\n",
    "    sum_numerator = np.sum(sum_pred_n_sqr)\n",
    "    \n",
    "    answer = sum_numerator/sum_denominator\n",
    "    print('PCA_' + str(pca) + ' PVE :' + str(answer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a easier way to do this:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the USArrests data. We will now perform hierarchical clustering on the states.\n",
    "\n",
    "- (a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.\n",
    "- (b) Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?\n",
    "- (c) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.\n",
    "- (d) What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do hierarchical clutering with complete linkage and Euclidean distance using Scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define non-scaled data\n",
    "X_raw = data_usarrests.loc[:, features].values\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "hc_usarrests = linkage(X_raw, method='complete', metric='euclidean')\n",
    "\n",
    "# Plot dedrogram\n",
    "states = data_usarrests.index.get_values()\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical clustering with complete linkage and euclidean distance - NON SCALED DATA', size=25)\n",
    "plt.xlabel('city', size=20)\n",
    "plt.ylabel('distance', size=20)\n",
    "dendrogram(hc_usarrests,leaf_rotation=90., leaf_font_size=15, labels=states)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut tree with three clusters\n",
    "clusters_3 = cut_tree(hc_usarrests, n_clusters=3)\n",
    "\n",
    "# Display which states belong to which clusters\n",
    "clusters_3_df = pd.DataFrame(clusters_3, index=states, columns=['Cluster'])\n",
    "print(clusters_3_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat these steps using scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering on scaled data\n",
    "hc_usarrests_scaled = linkage(X, method='complete', metric='euclidean')\n",
    "\n",
    "# Plot dedrogram\n",
    "states = data_usarrests.index.get_values()\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical clustering with complete linkage and euclidean distance - SCALED DATA', size=25)\n",
    "plt.xlabel('city', size=20)\n",
    "plt.ylabel('distance', size=20)\n",
    "dendrogram(hc_usarrests,leaf_rotation=90., leaf_font_size=15, labels=states)\n",
    "plt.show()\n",
    "\n",
    "# Cut tree with three clusters\n",
    "clusters_3_scaled = cut_tree(hc_usarrests_scaled, n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Non-scaled cluster  Scaled cluster\n",
      "Alabama                          0               0\n",
      "Alaska                           0               0\n",
      "Arizona                          0               1\n",
      "Arkansas                         1               2\n",
      "California                       0               1\n",
      "Colorado                         1               1\n",
      "Connecticut                      2               2\n",
      "Delaware                         0               2\n",
      "Florida                          0               1\n",
      "Georgia                          1               0\n",
      "Hawaii                           2               2\n",
      "Idaho                            2               2\n",
      "Illinois                         0               1\n",
      "Indiana                          2               2\n",
      "Iowa                             2               2\n",
      "Kansas                           2               2\n",
      "Kentucky                         2               2\n",
      "Louisiana                        0               0\n",
      "Maine                            2               2\n",
      "Maryland                         0               1\n",
      "Massachusetts                    1               2\n",
      "Michigan                         0               1\n",
      "Minnesota                        2               2\n",
      "Mississippi                      0               0\n",
      "Missouri                         1               2\n",
      "Montana                          2               2\n",
      "Nebraska                         2               2\n",
      "Nevada                           0               1\n",
      "New Hampshire                    2               2\n",
      "New Jersey                       1               2\n",
      "New Mexico                       0               1\n",
      "New York                         0               1\n",
      "North Carolina                   0               0\n",
      "North Dakota                     2               2\n",
      "Ohio                             2               2\n",
      "Oklahoma                         1               2\n",
      "Oregon                           1               2\n",
      "Pennsylvania                     2               2\n",
      "Rhode Island                     1               2\n",
      "South Carolina                   0               0\n",
      "South Dakota                     2               2\n",
      "Tennessee                        1               0\n",
      "Texas                            1               1\n",
      "Utah                             2               2\n",
      "Vermont                          2               2\n",
      "Virginia                         1               2\n",
      "Washington                       1               2\n",
      "West Virginia                    2               2\n",
      "Wisconsin                        2               2\n",
      "Wyoming                          1               2\n"
     ]
    }
   ],
   "source": [
    "# Display which states belong to which clusters, comparing scaled and non-scaled data\n",
    "clusters = np.append(clusters_3, clusters_3_scaled, axis=1)\n",
    "\n",
    "clusters_3_df = pd.DataFrame(clusters,\n",
    "                             index = states,\n",
    "                             columns = ['Non-scaled cluster', 'Scaled cluster']\n",
    "                             )\n",
    "\n",
    "print(clusters_3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
